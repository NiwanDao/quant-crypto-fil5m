# model_validation.py
import pandas as pd
import numpy as np
import joblib
import json
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, 
    precision_recall_curve, f1_score, precision_score, recall_score
)
from sklearn.model_selection import TimeSeriesSplit
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Tuple, List
import yaml

class ModelValidator:
    def __init__(self, model_path: str, thresholds_path: str, config_path: str):
        self.models = joblib.load(model_path)
        with open(thresholds_path, 'r') as f:
            self.thresholds = json.load(f)
        self.config = self.load_config(config_path)
        
    def load_config(self, config_path: str):
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    
    def basic_performance_validation(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """åŸºç¡€æ€§èƒ½éªŒè¯ - ä¿®å¤ç‰ˆæœ¬"""
        print("ğŸ§ª åŸºç¡€æ€§èƒ½éªŒè¯...")
        
        # é›†æˆé¢„æµ‹
        y_proba = self.ensemble_predict(X)
        
        # å®‰å…¨åœ°è®¡ç®—æŒ‡æ ‡
        try:
            y_pred_buy = (y_proba > self.thresholds['buy_threshold']).astype(int)
            y_pred_sell = (y_proba < self.thresholds['sell_threshold']).astype(int)
            
            metrics = {
                'auc': roc_auc_score(y, y_proba),
                'f1_buy': f1_score(y, y_pred_buy, zero_division=0),
                'f1_sell': f1_score(1-y, y_pred_sell, zero_division=0),
                'precision_buy': precision_score(y, y_pred_buy, zero_division=0),
                'recall_buy': recall_score(y, y_pred_buy, zero_division=0),
                'precision_sell': precision_score(1-y, y_pred_sell, zero_division=0),
                'recall_sell': recall_score(1-y, y_pred_sell, zero_division=0),
                'buy_signals_ratio': y_pred_buy.mean(),
                'sell_signals_ratio': y_pred_sell.mean()
            }
            
        except Exception as e:
            print(f"âŒ åŸºç¡€æ€§èƒ½éªŒè¯å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å€¼
            metrics = {
                'auc': 0.5,
                'f1_buy': 0.0,
                'f1_sell': 0.0,
                'precision_buy': 0.0,
                'recall_buy': 0.0,
                'precision_sell': 0.0,
                'recall_sell': 0.0,
                'buy_signals_ratio': 0.0,
                'sell_signals_ratio': 0.0
            }
        
        print("ğŸ“Š åŸºç¡€æ€§èƒ½æŒ‡æ ‡:")
        for key, value in metrics.items():
            print(f"  {key}: {value:.4f}")
        
        return metrics
    def time_series_cross_validation(self, X: pd.DataFrame, y: pd.Series, n_splits: int = 5) -> Dict:
        """æ—¶é—´åºåˆ—äº¤å‰éªŒè¯"""
        print("ğŸ”„ æ—¶é—´åºåˆ—äº¤å‰éªŒè¯...")
        
        tscv = TimeSeriesSplit(n_splits=n_splits)
        cv_metrics = []
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]
            
            y_proba = self.ensemble_predict(X_val)
            y_pred = (y_proba > self.thresholds['buy_threshold']).astype(int)
            
            fold_metrics = {
                'fold': fold + 1,
                'auc': roc_auc_score(y_val, y_proba),
                'f1': f1_score(y_val, y_pred),
                'precision': precision_score(y_val, y_pred),
                'recall': recall_score(y_val, y_pred),
                'signals_ratio': y_pred.mean()
            }
            cv_metrics.append(fold_metrics)
            
            print(f"  æŠ˜å  {fold + 1}: AUC={fold_metrics['auc']:.4f}, F1={fold_metrics['f1']:.4f}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        cv_df = pd.DataFrame(cv_metrics)
        summary = {
            'mean_auc': cv_df['auc'].mean(),
            'std_auc': cv_df['auc'].std(),
            'mean_f1': cv_df['f1'].mean(),
            'std_f1': cv_df['f1'].std(),
            'stability_score': cv_df['auc'].mean() / (cv_df['auc'].std() + 1e-8)
        }
        
        print(f"ğŸ“ˆ äº¤å‰éªŒè¯æ€»ç»“:")
        print(f"  AUC: {summary['mean_auc']:.4f} Â± {summary['std_auc']:.4f}")
        print(f"  F1: {summary['mean_f1']:.4f} Â± {summary['std_f1']:.4f}")
        print(f"  ç¨³å®šæ€§åˆ†æ•°: {summary['stability_score']:.4f}")
        
        return summary
    
    def threshold_sensitivity_analysis(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """é˜ˆå€¼æ•æ„Ÿæ€§åˆ†æ - ä¿®å¤ç‰ˆæœ¬"""
        print("ğŸ¯ é˜ˆå€¼æ•æ„Ÿæ€§åˆ†æ...")
        
        y_proba = self.ensemble_predict(X)
        
        # è°ƒæ•´é˜ˆå€¼èŒƒå›´ï¼Œç¡®ä¿åŒ…å«å®é™…ä½¿ç”¨çš„é˜ˆå€¼
        current_threshold = self.thresholds['buy_threshold']
        
        # åŠ¨æ€è®¾ç½®é˜ˆå€¼èŒƒå›´ï¼Œå›´ç»•å½“å‰é˜ˆå€¼
        threshold_min = max(0.1, current_threshold - 0.2)
        threshold_max = min(0.9, current_threshold + 0.2)
        
        thresholds = np.arange(threshold_min, threshold_max, 0.05)
        
        # ç¡®ä¿åŒ…å«å½“å‰é˜ˆå€¼
        if current_threshold not in thresholds:
            thresholds = np.sort(np.append(thresholds, current_threshold))
        
        results = []
        
        for threshold in thresholds:
            y_pred = (y_proba > threshold).astype(int)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ ·æœ¬è¿›è¡Œè®¡ç®—
            if len(np.unique(y_pred)) < 2:
                # å¦‚æœåªæœ‰å•ä¸€é¢„æµ‹ï¼Œè·³è¿‡æˆ–è®¾ç½®é»˜è®¤å€¼
                continue
            
            try:
                result = {
                    'threshold': threshold,
                    'f1': f1_score(y, y_pred, zero_division=0),
                    'precision': precision_score(y, y_pred, zero_division=0),
                    'recall': recall_score(y, y_pred, zero_division=0),
                    'signals_ratio': y_pred.mean(),
                    'accuracy': (y_pred == y).mean()
                }
                results.append(result)
            except Exception as e:
                print(f"âš ï¸ é˜ˆå€¼ {threshold:.4f} è®¡ç®—å¤±è´¥: {e}")
                continue
        
        if not results:
            print("âŒ æ— æ³•è¿›è¡Œé˜ˆå€¼åˆ†æ")
            return {
                'current_threshold': current_threshold,
                'optimal_threshold': current_threshold,
                'improvement': 0.0,
                'error': 'æ— æ³•è®¡ç®—é˜ˆå€¼æ•æ„Ÿæ€§'
            }
        
        results_df = pd.DataFrame(results)
        
        # æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼
        if not results_df.empty:
            optimal_idx = results_df['f1'].idxmax()
            optimal_threshold = results_df.loc[optimal_idx, 'threshold']
            optimal_f1 = results_df.loc[optimal_idx, 'f1']
            
            # å®‰å…¨åœ°æŸ¥æ‰¾å½“å‰é˜ˆå€¼çš„F1åˆ†æ•°
            current_threshold_data = results_df[results_df['threshold'] == current_threshold]
            
            if not current_threshold_data.empty:
                current_f1 = current_threshold_data['f1'].iloc[0]
            else:
                # å¦‚æœå½“å‰é˜ˆå€¼ä¸åœ¨ç»“æœä¸­ï¼Œæ‰¾åˆ°æœ€æ¥è¿‘çš„é˜ˆå€¼
                closest_idx = (results_df['threshold'] - current_threshold).abs().idxmin()
                closest_threshold = results_df.loc[closest_idx, 'threshold']
                current_f1 = results_df.loc[closest_idx, 'f1']
                print(f"âš ï¸ å½“å‰é˜ˆå€¼ {current_threshold:.4f} ä¸åœ¨æµ‹è¯•èŒƒå›´ï¼Œä½¿ç”¨æœ€æ¥è¿‘å€¼ {closest_threshold:.4f}")
            
            improvement = optimal_f1 - current_f1
            
            print(f"ğŸ” å½“å‰é˜ˆå€¼: {current_threshold:.4f} (F1={current_f1:.4f})")
            print(f"ğŸ¯ æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.4f} (F1={optimal_f1:.4f})")
            print(f"ğŸ“ˆ F1æ”¹è¿›: {improvement:.4f}")
            
            # ç»˜åˆ¶æ•æ„Ÿæ€§åˆ†æå›¾
            self._plot_threshold_sensitivity(results_df, current_threshold, optimal_threshold)
            
            return {
                'current_threshold': current_threshold,
                'optimal_threshold': optimal_threshold,
                'improvement': improvement,
                'current_f1': current_f1,
                'optimal_f1': optimal_f1
            }
        else:
            print("âŒ æ— æ³•è®¡ç®—æœ€ä¼˜é˜ˆå€¼")
            return {
                'current_threshold': current_threshold,
                'optimal_threshold': current_threshold,
                'improvement': 0.0,
                'error': 'æ— æ³•è®¡ç®—æœ€ä¼˜é˜ˆå€¼'
            }

    def _plot_threshold_sensitivity(self, results_df: pd.DataFrame, current_threshold: float, optimal_threshold: float):
        """ç»˜åˆ¶é˜ˆå€¼æ•æ„Ÿæ€§åˆ†æå›¾ - è¾…åŠ©å‡½æ•°"""
        try:
            plt.figure(figsize=(12, 8))
            
            # åˆ›å»ºå­å›¾
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
            
            # ç¬¬ä¸€ä¸ªå­å›¾ï¼šæ€§èƒ½æŒ‡æ ‡
            ax1.plot(results_df['threshold'], results_df['f1'], label='F1 Score', marker='o', linewidth=2, color='blue')
            ax1.plot(results_df['threshold'], results_df['precision'], label='Precision', marker='s', alpha=0.7, color='green')
            ax1.plot(results_df['threshold'], results_df['recall'], label='Recall', marker='^', alpha=0.7, color='orange')
            
            ax1.axvline(current_threshold, color='red', linestyle='--', 
                    label=f'Current ({current_threshold:.3f})', alpha=0.8, linewidth=2)
            ax1.axvline(optimal_threshold, color='purple', linestyle='--', 
                    label=f'Optimal ({optimal_threshold:.3f})', alpha=0.8, linewidth=2)
            
            ax1.set_xlabel('Threshold')
            ax1.set_ylabel('Score')
            ax1.set_title('Threshold Sensitivity Analysis - Performance Metrics')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # ç¬¬äºŒä¸ªå­å›¾ï¼šä¿¡å·æ¯”ä¾‹å’Œå‡†ç¡®ç‡
            ax2.plot(results_df['threshold'], results_df['signals_ratio'], 
                    label='Signals Ratio', marker='d', color='brown', linewidth=2)
            ax2.plot(results_df['threshold'], results_df['accuracy'], 
                    label='Accuracy', marker='x', color='gray', linewidth=2)
            
            ax2.axvline(current_threshold, color='red', linestyle='--', alpha=0.8)
            ax2.axvline(optimal_threshold, color='purple', linestyle='--', alpha=0.8)
            
            ax2.set_xlabel('Threshold')
            ax2.set_ylabel('Ratio / Accuracy')
            ax2.set_title('Threshold Sensitivity Analysis - Signals and Accuracy')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾ç‰‡
            import os
            os.makedirs('validation', exist_ok=True)
            plt.savefig('validation/threshold_sensitivity.png', dpi=150, bbox_inches='tight')
            plt.close()
            
            print("âœ… é˜ˆå€¼æ•æ„Ÿæ€§åˆ†æå›¾å·²ä¿å­˜")
            
        except Exception as e:
            print(f"âš ï¸ ç»˜åˆ¶é˜ˆå€¼æ•æ„Ÿæ€§åˆ†æå›¾å¤±è´¥: {e}")
        
    def feature_importance_analysis(self, X: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾é‡è¦æ€§åˆ†æ"""
        print("ğŸ” ç‰¹å¾é‡è¦æ€§åˆ†æ...")
        
        # è®¡ç®—å¹³å‡ç‰¹å¾é‡è¦æ€§
        importance_data = []
        for i, model in enumerate(self.models):
            importance = model.feature_importances_
            for j, (feature, imp) in enumerate(zip(X.columns, importance)):
                importance_data.append({
                    'feature': feature,
                    'importance': imp,
                    'model': f'model_{i+1}'
                })
        
        importance_df = pd.DataFrame(importance_data)
        avg_importance = importance_df.groupby('feature')['importance'].mean().sort_values(ascending=False)
        
        # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾
        plt.figure(figsize=(12, 8))
        avg_importance.head(20).plot(kind='barh')
        plt.title('Top 20 Feature Importance')
        plt.xlabel('Average Importance')
        plt.tight_layout()
        plt.savefig('validation/feature_importance.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        print("ğŸ† Top 10 é‡è¦ç‰¹å¾:")
        for i, (feature, importance) in enumerate(avg_importance.head(10).items()):
            print(f"  {i+1:2d}. {feature}: {importance:.4f}")
        
        return avg_importance
    
    def model_consistency_check(self, X: pd.DataFrame) -> Dict:
        """æ¨¡å‹ä¸€è‡´æ€§æ£€æŸ¥"""
        print("ğŸ”„ æ¨¡å‹ä¸€è‡´æ€§æ£€æŸ¥...")
        
        # è®¡ç®—æ¨¡å‹é—´é¢„æµ‹çš„ç›¸å…³æ€§
        predictions = []
        for model in self.models:
            pred = model.predict_proba(X)[:, 1]
            predictions.append(pred)
        
        predictions_array = np.array(predictions)
        correlation_matrix = np.corrcoef(predictions_array)
        
        # è®¡ç®—å¹³å‡ç›¸å…³æ€§ï¼ˆæ’é™¤å¯¹è§’çº¿ï¼‰
        mask = ~np.eye(correlation_matrix.shape[0], dtype=bool)
        avg_correlation = correlation_matrix[mask].mean()
        
        # è®¡ç®—é¢„æµ‹æ ‡å‡†å·®
        pred_std = predictions_array.std(axis=0).mean()
        
        consistency_metrics = {
            'avg_model_correlation': avg_correlation,
            'avg_prediction_std': pred_std,
            'consistency_score': 1 - pred_std  # æ ‡å‡†å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šå¥½
        }
        
        print(f"ğŸ“Š æ¨¡å‹ä¸€è‡´æ€§æŒ‡æ ‡:")
        print(f"  å¹³å‡æ¨¡å‹é—´ç›¸å…³æ€§: {consistency_metrics['avg_model_correlation']:.4f}")
        print(f"  å¹³å‡é¢„æµ‹æ ‡å‡†å·®: {consistency_metrics['avg_prediction_std']:.4f}")
        print(f"  ä¸€è‡´æ€§åˆ†æ•°: {consistency_metrics['consistency_score']:.4f}")
        
        return consistency_metrics
    
    def ensemble_predict(self, X: pd.DataFrame) -> np.ndarray:
        """é›†æˆæ¨¡å‹é¢„æµ‹"""
        predictions = []
        for model in self.models:
            pred = model.predict_proba(X)[:, 1]
            predictions.append(pred)
        return np.mean(predictions, axis=0)
    
    def run_comprehensive_validation(self, X: pd.DataFrame, y: pd.Series, output_dir: str = 'validation'):
        """è¿è¡Œå…¨é¢éªŒè¯"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        print("ğŸ¯ å¼€å§‹å…¨é¢æ¨¡å‹éªŒè¯...")
        print("=" * 50)
        
        validation_results = {}
        
        # 1. åŸºç¡€æ€§èƒ½éªŒè¯
        validation_results['basic_performance'] = self.basic_performance_validation(X, y)
        print("-" * 30)
        
        # 2. æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        validation_results['cross_validation'] = self.time_series_cross_validation(X, y)
        print("-" * 30)
        
        # 3. é˜ˆå€¼æ•æ„Ÿæ€§åˆ†æ
        validation_results['threshold_analysis'] = self.threshold_sensitivity_analysis(X, y)
        print("-" * 30)
        
        # 4. ç‰¹å¾é‡è¦æ€§åˆ†æ
        validation_results['feature_importance'] = self.feature_importance_analysis(X)
        print("-" * 30)
        
        # 5. æ¨¡å‹ä¸€è‡´æ€§æ£€æŸ¥
        validation_results['model_consistency'] = self.model_consistency_check(X)
        print("-" * 30)
        
        # ä¿å­˜éªŒè¯ç»“æœ
        self.save_validation_results(validation_results, output_dir)
        
        # ç”ŸæˆéªŒè¯æŠ¥å‘Š
        self.generate_validation_report(validation_results, output_dir)
        
        return validation_results
    
    def save_validation_results(self, results: Dict, output_dir: str):
        """ä¿å­˜éªŒè¯ç»“æœ"""
        # ä¿å­˜åŸºç¡€æŒ‡æ ‡
        basic_metrics = results['basic_performance']
        pd.DataFrame([basic_metrics]).to_csv(f'{output_dir}/basic_metrics.csv', index=False)
        
        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        results['feature_importance'].to_csv(f'{output_dir}/feature_importance.csv')
        
        print(f"ğŸ’¾ éªŒè¯ç»“æœå·²ä¿å­˜åˆ° {output_dir}/ ç›®å½•")
    
    def generate_validation_report(self, results: Dict, output_dir: str):
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("             æ¨¡å‹éªŒè¯æŠ¥å‘Š")
        report.append("=" * 60)
        
        # åŸºç¡€æ€§èƒ½
        bp = results['basic_performance']
        report.append("\nğŸ“Š åŸºç¡€æ€§èƒ½æŒ‡æ ‡:")
        report.append(f"  AUC: {bp['auc']:.4f}")
        report.append(f"  ä¹°å…¥F1: {bp['f1_buy']:.4f}")
        report.append(f"  å–å‡ºF1: {bp['f1_sell']:.4f}")
        report.append(f"  ä¹°å…¥ä¿¡å·æ¯”ä¾‹: {bp['buy_signals_ratio']:.4f}")
        
        # äº¤å‰éªŒè¯
        cv = results['cross_validation']
        report.append("\nğŸ”„ äº¤å‰éªŒè¯ç»“æœ:")
        report.append(f"  å¹³å‡AUC: {cv['mean_auc']:.4f} Â± {cv['std_auc']:.4f}")
        report.append(f"  ç¨³å®šæ€§åˆ†æ•°: {cv['stability_score']:.4f}")
        
        # é˜ˆå€¼åˆ†æ
        ta = results['threshold_analysis']
        report.append("\nğŸ¯ é˜ˆå€¼åˆ†æ:")
        report.append(f"  å½“å‰é˜ˆå€¼: {ta['current_threshold']:.4f}")
        report.append(f"  å»ºè®®é˜ˆå€¼: {ta['optimal_threshold']:.4f}")
        report.append(f"  F1æå‡: {ta['improvement']:.4f}")
        
        # æ¨¡å‹ä¸€è‡´æ€§
        mc = results['model_consistency']
        report.append("\nğŸ”„ æ¨¡å‹ä¸€è‡´æ€§:")
        report.append(f"  å¹³å‡ç›¸å…³æ€§: {mc['avg_model_correlation']:.4f}")
        report.append(f"  ä¸€è‡´æ€§åˆ†æ•°: {mc['consistency_score']:.4f}")
        
        # è¯„ä¼°ç»“è®º
        report.append("\nğŸ“ˆ è¯„ä¼°ç»“è®º:")
        if bp['auc'] > 0.7:
            report.append("  âœ… æ¨¡å‹æ€§èƒ½ä¼˜ç§€")
        elif bp['auc'] > 0.6:
            report.append("  âš ï¸ æ¨¡å‹æ€§èƒ½è‰¯å¥½")
        else:
            report.append("  âŒ æ¨¡å‹æ€§èƒ½éœ€è¦æ”¹è¿›")
        
        if cv['stability_score'] > 10:
            report.append("  âœ… æ¨¡å‹ç¨³å®šæ€§ä¼˜ç§€")
        else:
            report.append("  âš ï¸ æ¨¡å‹ç¨³å®šæ€§éœ€è¦å…³æ³¨")
        
        # ä¿å­˜æŠ¥å‘Š
        report_text = '\n'.join(report)
        with open(f'{output_dir}/validation_report.txt', 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print(report_text)

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # åŠ è½½æ•°æ®
    df = pd.read_parquet('data/feat_2025_3_to_2025_6.parquet')
    features = [c for c in df.columns if c not in ['y']]
    X, y = df[features], df['y']
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = ModelValidator(
        model_path='models/ensemble_models.pkl',
        thresholds_path='models/optimal_thresholds.json',
        config_path='conf/config.yml'
    )
    
    # è¿è¡Œå…¨é¢éªŒè¯
    results = validator.run_comprehensive_validation(X, y)

if __name__ == '__main__':
    main()