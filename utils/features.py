import pandas as pd
import numpy as np
from ta.momentum import RSIIndicator, StochasticOscillator, ROCIndicator
from ta.trend import EMAIndicator, MACD, ADXIndicator, IchimokuIndicator
from ta.volatility import BollingerBands, AverageTrueRange, DonchianChannel
from ta.volume import OnBalanceVolumeIndicator, AccDistIndexIndicator
from ta.others import DailyReturnIndicator


def build_features_safe(df: pd.DataFrame) -> pd.DataFrame:
    """
    å®‰å…¨ç‰ˆæœ¬çš„ç‰¹å¾å·¥ç¨‹ - é¿å…æ•°æ®æ³„éœ²
    """
    df = df.copy()
    
    # ç¡®ä¿æŒ‰æ—¶é—´é¡ºåºå¤„ç†
    df = df.sort_index()
    
    # 1. åŸºç¡€ä»·æ ¼ç‰¹å¾ - åªä½¿ç”¨å†å²ä¿¡æ¯
    df['returns'] = df['close'].pct_change()
    df['returns_lag1'] = df['returns'].shift(1)  # ä½¿ç”¨æ»åå€¼
    df['high_low_ratio'] = df['high'] / df['low']
    df['open_close_ratio'] = df['close'] / df['open']
    df['body_size'] = abs(df['close'] - df['open']) / df['open']
    
    # 2. è¶‹åŠ¿ç‰¹å¾ - ä½¿ç”¨æ»åå€¼é¿å…æœªæ¥ä¿¡æ¯
    for period in [5, 10, 20]:
        # ä½¿ç”¨shift(1)ç¡®ä¿åªä½¿ç”¨å†å²æ•°æ®è®¡ç®—EMA
        df[f'ema_{period}'] = df['close'].shift(1).ewm(span=period).mean()
        df[f'price_ema_ratio_{period}'] = df['close'] / df[f'ema_{period}']
    
    # 3. MACD - ä½¿ç”¨å†å²æ•°æ®è®¡ç®—
    def safe_macd(close_series, fast=12, slow=26, signal=9):
        """å®‰å…¨çš„MACDè®¡ç®—"""
        ema_fast = close_series.shift(1).ewm(span=fast).mean()
        ema_slow = close_series.shift(1).ewm(span=slow).mean()
        macd_line = ema_fast - ema_slow
        signal_line = macd_line.ewm(span=signal).mean()
        histogram = macd_line - signal_line
        return macd_line, signal_line, histogram
    
    macd_line, signal_line, histogram = safe_macd(df['close'])
    df['macd'] = macd_line
    df['macd_signal'] = signal_line
    df['macd_histogram'] = histogram
    
    # 4. RSI - ä½¿ç”¨å†å²æ•°æ®è®¡ç®—
    def safe_rsi(series, window=14):
        """å®‰å…¨çš„RSIè®¡ç®—"""
        # ä½¿ç”¨shiftç¡®ä¿æ²¡æœ‰æœªæ¥ä¿¡æ¯
        deltas = series.shift(1).diff()
        gains = deltas.where(deltas > 0, 0)
        losses = -deltas.where(deltas < 0, 0)
        
        avg_gains = gains.rolling(window=window, min_periods=1).mean()
        avg_losses = losses.rolling(window=window, min_periods=1).mean()
        
        rs = avg_gains / (avg_losses + 1e-10)
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    for period in [7, 14]:
        df[f'rsi_{period}_safe'] = safe_rsi(df['close'], period)
    
    # 5. å¸ƒæ—å¸¦ - ä½¿ç”¨å†å²æ•°æ®
    def safe_bollinger_bands(close_series, window=20, num_std=2):
        """å®‰å…¨çš„å¸ƒæ—å¸¦è®¡ç®—"""
        rolling_mean = close_series.shift(1).rolling(window=window).mean()
        rolling_std = close_series.shift(1).rolling(window=window).std()
        
        upper_band = rolling_mean + (rolling_std * num_std)
        lower_band = rolling_mean - (rolling_std * num_std)
        
        return upper_band, lower_band, rolling_mean
    
    bb_upper, bb_lower, bb_middle = safe_bollinger_bands(df['close'])
    df['bb_upper_safe'] = bb_upper
    df['bb_lower_safe'] = bb_lower
    df['bb_middle_safe'] = bb_middle
    df['bb_position_safe'] = (df['close'] - bb_lower) / (bb_upper - bb_lower)
    
    # 6. æˆäº¤é‡ç‰¹å¾ - ä½¿ç”¨æ»åå€¼
    df['volume_lag1'] = df['volume'].shift(1)
    df['volume_ratio_safe'] = df['volume_lag1'] / df['volume_lag1'].rolling(20).mean()
    
    # 7. ä»·æ ¼ä½ç½®ç‰¹å¾ - ä½¿ç”¨å†å²é«˜ä½ç‚¹
    df['resistance_20'] = df['high'].shift(1).rolling(20).max()  # ä½¿ç”¨shift(1)
    df['support_20'] = df['low'].shift(1).rolling(20).min()     # ä½¿ç”¨shift(1)
    df['breakout_high'] = (df['close'] > df['resistance_20']).astype(int)
    df['breakout_low'] = (df['close'] < df['support_20']).astype(int)
    
    # æ¸…ç†æ•°æ®
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df = df.dropna()
    
    print(f"âœ… å®‰å…¨ç‰¹å¾æ„å»ºå®Œæˆ: {len(df.columns)} ä¸ªç‰¹å¾")
    print(f"ğŸ“Š æœ‰æ•ˆæ ·æœ¬æ•°: {len(df)}")
    
    return df


def detect_data_leakage(df, target_col='y'):
    """æ£€æµ‹æ•°æ®æ³„éœ²"""
    print("ğŸ” æ£€æµ‹æ•°æ®æ³„éœ²...")
    
    # æ£€æŸ¥ç‰¹å¾ä¸æ ‡ç­¾çš„ç›¸å…³æ€§
    feature_cols = [col for col in df.columns if col != target_col]
    correlations = []
    
    for col in feature_cols:
        corr = df[col].corr(df[target_col])
        correlations.append((col, corr))
    
    # æ’åºå¹¶æ˜¾ç¤ºé«˜ç›¸å…³æ€§ç‰¹å¾
    correlations.sort(key=lambda x: abs(x[1]), reverse=True)
    
    print("ğŸ“Š ç‰¹å¾ä¸æ ‡ç­¾ç›¸å…³æ€§Top 10:")
    suspicious_features = []
    for col, corr in correlations[:10]:
        print(f"  {col}: {corr:.4f}")
        if abs(corr) > 0.8:
            suspicious_features.append((col, corr))
    
    if suspicious_features:
        print("âŒ å‘ç°å¯ç–‘çš„é«˜ç›¸å…³æ€§ç‰¹å¾ï¼ˆå¯èƒ½æ•°æ®æ³„éœ²ï¼‰:")
        for col, corr in suspicious_features:
            print(f"  âš ï¸ {col}: {corr:.4f}")
        return False
    else:
        print("âœ… æœªå‘ç°æ˜æ˜¾æ•°æ®æ³„éœ²")
        return True

def build_labels_safe(df: pd.DataFrame, forward_n: int = 4, thr: float = 0.008) -> pd.DataFrame:
    """
    ä¿®å¤ç‰ˆæ ‡ç­¾ç”Ÿæˆ - åˆ›å»ºæ›´åˆç†çš„äº¤æ˜“ä¿¡å·
    """
    df = df.copy()
    
    # æ–¹æ³•1: ç®€å•æ”¶ç›Šç‡æ ‡ç­¾ (ä¿æŒå…¼å®¹æ€§)
    future_return = df['close'].pct_change(forward_n).shift(-forward_n)
    df['y_simple'] = (future_return > thr).astype(int)
    
    # æ–¹æ³•2: æ”¹è¿›çš„æ³¢åŠ¨ç‡è°ƒæ•´æ ‡ç­¾
    volatility = df['returns'].rolling(forward_n).std()
    adaptive_threshold = thr * (volatility / volatility.median())  # æ ¹æ®æ³¢åŠ¨ç‡è°ƒæ•´é˜ˆå€¼
    df['y_vol_adjusted'] = (future_return > adaptive_threshold).astype(int)
    
    # æ–¹æ³•3: ç›¸å¯¹å¼ºåº¦æ ‡ç­¾ (æ¨è)
    future_max = df['close'].shift(-forward_n).rolling(forward_n).max()
    future_min = df['close'].shift(-forward_n).rolling(forward_n).min()
    
    # ä¹°å…¥ä¿¡å·: æœªæ¥æœ€é«˜ä»·æ¯”å½“å‰ä»·ä¸Šæ¶¨è¶…è¿‡é˜ˆå€¼
    buy_signal = (future_max / df['close'] - 1) > thr
    
    # å–å‡ºä¿¡å·: æœªæ¥æœ€ä½ä»·æ¯”å½“å‰ä»·ä¸‹è·Œè¶…è¿‡é˜ˆå€¼  
    sell_signal = (1 - future_min / df['close']) > thr
    
    # ä¸‰åˆ†ç±»æ ‡ç­¾
    df['y_tri'] = 0  # 0: æŒæœ‰
    df.loc[buy_signal & ~sell_signal, 'y_tri'] = 1  # 1: ä¹°å…¥
    df.loc[sell_signal & ~buy_signal, 'y_tri'] = 2  # 2: å–å‡º
    
    # ä½¿ç”¨ä¸‰åˆ†ç±»ä¸­çš„ä¹°å…¥ä¿¡å·ä½œä¸ºä¸»è¦æ ‡ç­¾
    df['y'] = (df['y_tri'] == 1).astype(int)
    
    # åˆ é™¤æœªæ¥æ•°æ®ä¸å¯ç”¨çš„è¡Œ
    df = df.dropna()
    
    # åˆ†ææ ‡ç­¾åˆ†å¸ƒ
    print("ğŸ¯ æ ‡ç­¾åˆ†å¸ƒåˆ†æ:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"  ä¹°å…¥ä¿¡å· (y=1): {(df['y'] == 1).sum()} ({(df['y'] == 1).mean():.2%})")
    print(f"  éä¹°å…¥ä¿¡å· (y=0): {(df['y'] == 0).sum()} ({(df['y'] == 0).mean():.2%})")
    
    if (df['y'] == 1).mean() > 0.7 or (df['y'] == 1).mean() < 0.3:
        print("âš ï¸  è­¦å‘Š: æ ‡ç­¾ä¸¥é‡ä¸å¹³è¡¡ï¼Œå»ºè®®è°ƒæ•´é˜ˆå€¼!")
        print(f"ğŸ’¡ å»ºè®®é˜ˆå€¼èŒƒå›´: {thr * 0.5:.4f} - {thr * 2:.4f}")
    
    return df


def build_multi_feature_labels(df: pd.DataFrame, forward_n: int = 4, base_thr: float = 0.008) -> pd.DataFrame:
    """
    åŸºäºå¤šä¸ªfeatureçš„æ™ºèƒ½æ ‡ç­¾ç”Ÿæˆç³»ç»Ÿ
    
    å‚æ•°:
    - df: åŒ…å«ç‰¹å¾çš„DataFrame
    - forward_n: å‘å‰é¢„æµ‹çš„å‘¨æœŸæ•°
    - base_thr: åŸºç¡€é˜ˆå€¼
    """
    df = df.copy()
    
    print("ğŸš€ å¼€å§‹åŸºäºå¤šç‰¹å¾çš„æ™ºèƒ½æ ‡ç­¾ç”Ÿæˆ...")
    
    # 1. è®¡ç®—æœªæ¥æ”¶ç›Šç‡
    future_return = df['close'].pct_change(forward_n).shift(-forward_n)
    
    # 2. ç‰¹å¾æƒé‡è®¡ç®— - åŸºäºå†å²è¡¨ç°
    feature_weights = calculate_feature_weights(df, future_return)
    
    # 3. åŠ¨æ€é˜ˆå€¼è®¡ç®—
    dynamic_thresholds = calculate_dynamic_thresholds(df, future_return, base_thr)
    
    # 4. å¤šç‰¹å¾ç»¼åˆè¯„åˆ†
    composite_scores = calculate_composite_scores(df, feature_weights)
    
    # 5. ç”Ÿæˆå¤šç§æ ‡ç­¾æ–¹æ³•
    labels = generate_ensemble_labels(df, future_return, composite_scores, dynamic_thresholds)
    
    # 6. æ ‡ç­¾èåˆå’Œä¼˜åŒ–
    final_labels = optimize_labels(df, labels, future_return)
    
    # 7. æ·»åŠ æ‰€æœ‰æ ‡ç­¾åˆ°DataFrame
    for label_name, label_values in final_labels.items():
        df[label_name] = label_values
    
    # åˆ é™¤åŒ…å«NaNçš„è¡Œ
    df = df.dropna()
    
    # 8. æ ‡ç­¾è´¨é‡åˆ†æ
    analyze_label_quality(df, future_return)
    
    print("âœ… å¤šç‰¹å¾æ ‡ç­¾ç”Ÿæˆå®Œæˆ!")
    return df


def calculate_feature_weights(df: pd.DataFrame, future_return: pd.Series) -> dict:
    """
    è®¡ç®—ç‰¹å¾æƒé‡ - åŸºäºä¸æœªæ¥æ”¶ç›Šç‡çš„ç›¸å…³æ€§
    """
    print("ğŸ“Š è®¡ç®—ç‰¹å¾æƒé‡...")
    
    # è·å–æ‰€æœ‰ç‰¹å¾åˆ—ï¼ˆæ’é™¤ä»·æ ¼å’Œæ ‡ç­¾ç›¸å…³åˆ—ï¼‰
    feature_cols = [col for col in df.columns 
                   if col not in ['open', 'high', 'low', 'close', 'volume', 'returns'] 
                   and not col.startswith('y_')]
    
    weights = {}
    correlations = []
    
    for col in feature_cols:
        if col in df.columns:
            # è®¡ç®—ä¸æœªæ¥æ”¶ç›Šç‡çš„ç›¸å…³æ€§
            corr = df[col].corr(future_return)
            if not np.isnan(corr):
                weights[col] = abs(corr)  # ä½¿ç”¨ç»å¯¹å€¼
                correlations.append((col, corr))
    
    # å½’ä¸€åŒ–æƒé‡
    total_weight = sum(weights.values())
    if total_weight > 0:
        weights = {k: v/total_weight for k, v in weights.items()}
    
    # æ˜¾ç¤ºTop 10æƒé‡ç‰¹å¾
    correlations.sort(key=lambda x: abs(x[1]), reverse=True)
    print("ğŸ† Top 10 ç‰¹å¾æƒé‡:")
    for i, (col, corr) in enumerate(correlations[:10]):
        weight = weights.get(col, 0)
        print(f"  {i+1:2d}. {col}: æƒé‡={weight:.4f}, ç›¸å…³æ€§={corr:.4f}")
    
    return weights


def calculate_dynamic_thresholds(df: pd.DataFrame, future_return: pd.Series, base_thr: float) -> dict:
    """
    è®¡ç®—åŠ¨æ€é˜ˆå€¼ - æ ¹æ®å¸‚åœºçŠ¶æ€è°ƒæ•´
    """
    print("ğŸ¯ è®¡ç®—åŠ¨æ€é˜ˆå€¼...")
    
    # è®¡ç®—å¸‚åœºçŠ¶æ€æŒ‡æ ‡
    volatility = df['returns'].rolling(20).std()
    trend_strength = abs(df['returns'].rolling(20).mean())
    volume_ratio = df['volume'] / df['volume'].rolling(20).mean()
    
    # å¸‚åœºçŠ¶æ€åˆ†ç±»
    high_vol = volatility > volatility.quantile(0.7)
    strong_trend = trend_strength > trend_strength.quantile(0.7)
    high_volume = volume_ratio > volume_ratio.quantile(0.7)
    
    # åŠ¨æ€é˜ˆå€¼è®¡ç®—
    thresholds = {}
    
    # åŸºç¡€é˜ˆå€¼
    thresholds['base'] = base_thr
    
    # é«˜æ³¢åŠ¨ç‡æ—¶é™ä½é˜ˆå€¼
    thresholds['vol_adjusted'] = np.where(high_vol, base_thr * 0.7, base_thr)
    
    # å¼ºè¶‹åŠ¿æ—¶æé«˜é˜ˆå€¼
    thresholds['trend_adjusted'] = np.where(strong_trend, base_thr * 1.3, base_thr)
    
    # é«˜æˆäº¤é‡æ—¶é™ä½é˜ˆå€¼
    thresholds['volume_adjusted'] = np.where(high_volume, base_thr * 0.8, base_thr)
    
    # ç»¼åˆè°ƒæ•´
    thresholds['composite'] = (thresholds['vol_adjusted'] + 
                              thresholds['trend_adjusted'] + 
                              thresholds['volume_adjusted']) / 3
    
    print(f"ğŸ“ˆ åŠ¨æ€é˜ˆå€¼èŒƒå›´: {np.min(thresholds['composite']):.4f} - {np.max(thresholds['composite']):.4f}")
    
    return thresholds


def calculate_composite_scores(df: pd.DataFrame, feature_weights: dict) -> pd.Series:
    """
    è®¡ç®—å¤šç‰¹å¾ç»¼åˆè¯„åˆ†
    """
    print("ğŸ§® è®¡ç®—ç»¼åˆè¯„åˆ†...")
    
    composite_score = pd.Series(0, index=df.index)
    
    for feature, weight in feature_weights.items():
        if feature in df.columns:
            # æ ‡å‡†åŒ–ç‰¹å¾å€¼
            feature_values = df[feature]
            if feature_values.std() > 0:
                normalized_values = (feature_values - feature_values.mean()) / feature_values.std()
                composite_score += normalized_values * weight
    
    # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
    if composite_score.std() > 0:
        composite_score = (composite_score - composite_score.min()) / (composite_score.max() - composite_score.min())
    
    print(f"ğŸ“Š ç»¼åˆè¯„åˆ†èŒƒå›´: {composite_score.min():.4f} - {composite_score.max():.4f}")
    
    return composite_score


def generate_ensemble_labels(df: pd.DataFrame, future_return: pd.Series, 
                           composite_scores: pd.Series, thresholds: dict) -> dict:
    """
    ç”Ÿæˆé›†æˆæ ‡ç­¾ - å¤šç§æ–¹æ³•ç»„åˆ
    """
    print("ğŸ­ ç”Ÿæˆé›†æˆæ ‡ç­¾...")
    
    labels = {}
    
    # æ–¹æ³•1: åŸºäºç»¼åˆè¯„åˆ†çš„æ ‡ç­¾
    score_threshold = composite_scores.quantile(0.7)  # å‰30%ä½œä¸ºä¹°å…¥ä¿¡å·
    labels['y_score_based'] = (composite_scores > score_threshold).astype(int)
    
    # æ–¹æ³•2: åŠ¨æ€é˜ˆå€¼æ ‡ç­¾
    labels['y_dynamic'] = (future_return > thresholds['composite']).astype(int)
    
    # æ–¹æ³•3: å¤šæ¡ä»¶ç»„åˆæ ‡ç­¾
    # æ¡ä»¶1: ç»¼åˆè¯„åˆ†é«˜
    condition1 = composite_scores > composite_scores.quantile(0.6)
    # æ¡ä»¶2: æœªæ¥æ”¶ç›Šç‡è¶…è¿‡åŠ¨æ€é˜ˆå€¼
    condition2 = future_return > thresholds['composite']
    # æ¡ä»¶3: æŠ€æœ¯æŒ‡æ ‡æ”¯æŒï¼ˆRSIä¸åœ¨è¶…ä¹°åŒºé—´ï¼‰
    rsi_cols = [col for col in df.columns if 'rsi' in col.lower()]
    condition3 = True
    if rsi_cols:
        rsi_values = df[rsi_cols[0]]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªRSI
        condition3 = rsi_values < 80  # ä¸åœ¨è¶…ä¹°åŒºé—´
    
    labels['y_multi_condition'] = (condition1 & condition2 & condition3).astype(int)
    
    # æ–¹æ³•4: ç›¸å¯¹å¼ºåº¦æ ‡ç­¾
    future_max = df['close'].shift(-4).rolling(4).max()
    future_min = df['close'].shift(-4).rolling(4).min()
    
    buy_signal = (future_max / df['close'] - 1) > thresholds['composite']
    sell_signal = (1 - future_min / df['close']) > thresholds['composite']
    
    labels['y_relative_strength'] = (buy_signal & ~sell_signal).astype(int)
    
    # æ–¹æ³•5: æ³¢åŠ¨ç‡è°ƒæ•´æ ‡ç­¾
    volatility = df['returns'].rolling(4).std()
    vol_adjusted_threshold = thresholds['composite'] * (volatility / volatility.median())
    labels['y_vol_adjusted'] = (future_return > vol_adjusted_threshold).astype(int)
    
    print(f"ğŸ“Š ç”Ÿæˆäº† {len(labels)} ç§æ ‡ç­¾æ–¹æ³•")
    
    return labels


def optimize_labels(df: pd.DataFrame, labels: dict, future_return: pd.Series) -> dict:
    """
    ä¼˜åŒ–æ ‡ç­¾è´¨é‡ - é€‰æ‹©æœ€ä½³ç»„åˆ
    """
    print("ğŸ”§ ä¼˜åŒ–æ ‡ç­¾è´¨é‡...")
    
    optimized_labels = {}
    
    # è®¡ç®—æ¯ç§æ ‡ç­¾çš„è´¨é‡æŒ‡æ ‡
    label_metrics = {}
    
    for label_name, label_values in labels.items():
        if len(label_values.dropna()) > 0:
            # è®¡ç®—å‡†ç¡®ç‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
            accuracy = (label_values == (future_return > 0)).mean()
            
            # è®¡ç®—æ ‡ç­¾åˆ†å¸ƒ
            positive_ratio = label_values.mean()
            
            # è®¡ç®—ä¸æœªæ¥æ”¶ç›Šç‡çš„ç›¸å…³æ€§
            correlation = label_values.corr(future_return)
            
            label_metrics[label_name] = {
                'accuracy': accuracy,
                'positive_ratio': positive_ratio,
                'correlation': correlation,
                'balance_score': 1 - abs(positive_ratio - 0.5) * 2  # å¹³è¡¡æ€§è¯„åˆ†
            }
    
    # é€‰æ‹©æœ€ä½³æ ‡ç­¾ç»„åˆ
    best_labels = []
    for label_name, metrics in label_metrics.items():
        # ç»¼åˆè¯„åˆ† = å‡†ç¡®ç‡ * ç›¸å…³æ€§ * å¹³è¡¡æ€§
        composite_score = (metrics['accuracy'] * 
                          abs(metrics['correlation']) * 
                          metrics['balance_score'])
        
        if composite_score > 0.1:  # æœ€ä½è´¨é‡é˜ˆå€¼
            best_labels.append((label_name, composite_score))
    
    # æŒ‰è¯„åˆ†æ’åº
    best_labels.sort(key=lambda x: x[1], reverse=True)
    
    print("ğŸ† æœ€ä½³æ ‡ç­¾æ–¹æ³•:")
    for i, (label_name, score) in enumerate(best_labels[:5]):
        metrics = label_metrics[label_name]
        print(f"  {i+1}. {label_name}: ç»¼åˆè¯„åˆ†={score:.4f}")
        print(f"     å‡†ç¡®ç‡={metrics['accuracy']:.4f}, ç›¸å…³æ€§={metrics['correlation']:.4f}")
        print(f"     æ­£æ ·æœ¬æ¯”ä¾‹={metrics['positive_ratio']:.4f}")
    
    # ç”Ÿæˆæœ€ç»ˆæ ‡ç­¾
    if best_labels:
        # ä½¿ç”¨æœ€ä½³æ ‡ç­¾ä½œä¸ºä¸»æ ‡ç­¾
        best_label_name = best_labels[0][0]
        optimized_labels['y'] = labels[best_label_name]
        
        # ä¿ç•™æ‰€æœ‰æ ‡ç­¾ä¾›é€‰æ‹©
        for label_name, _ in best_labels:
            optimized_labels[label_name] = labels[label_name]
    
    return optimized_labels


def analyze_label_quality(df: pd.DataFrame, future_return: pd.Series):
    """
    åˆ†ææ ‡ç­¾è´¨é‡
    """
    print("\nğŸ“Š æ ‡ç­¾è´¨é‡åˆ†æ:")
    print("=" * 50)
    
    label_cols = [col for col in df.columns if col.startswith('y')]
    
    for col in label_cols:
        if col in df.columns:
            label_values = df[col]
            if len(label_values.dropna()) > 0:
                # åŸºæœ¬ç»Ÿè®¡
                total_samples = len(label_values)
                positive_samples = (label_values == 1).sum()
                positive_ratio = positive_samples / total_samples
                
                # ä¸æœªæ¥æ”¶ç›Šç‡çš„ç›¸å…³æ€§
                correlation = label_values.corr(future_return)
                
                # é¢„æµ‹å‡†ç¡®æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰
                accuracy = (label_values == (future_return > 0)).mean()
                
                print(f"\nğŸ¯ {col}:")
                print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
                print(f"  æ­£æ ·æœ¬æ•°: {positive_samples} ({positive_ratio:.2%})")
                print(f"  ä¸æœªæ¥æ”¶ç›Šç‡ç›¸å…³æ€§: {correlation:.4f}")
                print(f"  é¢„æµ‹å‡†ç¡®æ€§: {accuracy:.4f}")
                
                # è´¨é‡è¯„ä¼°
                if abs(correlation) > 0.1 and 0.3 < positive_ratio < 0.7:
                    print(f"  âœ… æ ‡ç­¾è´¨é‡: è‰¯å¥½")
                elif abs(correlation) > 0.05:
                    print(f"  âš ï¸  æ ‡ç­¾è´¨é‡: ä¸€èˆ¬")
                else:
                    print(f"  âŒ æ ‡ç­¾è´¨é‡: è¾ƒå·®")
    
    print("\n" + "=" * 50)

# ç‰¹å¾é€‰æ‹©å‡½æ•° - åˆ é™¤ä½è´¨é‡ç‰¹å¾
def select_important_features(df, target_col='y', top_k=30):
    """
    é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾
    """
    from sklearn.ensemble import RandomForestClassifier
    
    X = df.drop(columns=[target_col, 'y_tri', 'y_simple', 'y_vol_adjusted'], errors='ignore')
    y = df[target_col]
    
    # ä½¿ç”¨éšæœºæ£®æ—è¿›è¡Œç‰¹å¾é‡è¦æ€§æ’åº
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X.fillna(0), y)
    
    # è·å–ç‰¹å¾é‡è¦æ€§
    importance_df = pd.DataFrame({
        'feature': X.columns,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    # é€‰æ‹©top_kç‰¹å¾
    selected_features = importance_df.head(top_k)['feature'].tolist()
    
    print(f"ğŸ¯ é€‰æ‹©äº† {len(selected_features)} ä¸ªæœ€é‡è¦ç‰¹å¾")
    print("ğŸ† Top 10 ç‰¹å¾:")
    for i, row in importance_df.head(10).iterrows():
        print(f"  {i+1:2d}. {row['feature']}: {row['importance']:.4f}")
    
    return df[selected_features + [target_col]]

# =============================================================================
# ä½¿ç”¨ç¤ºä¾‹å’Œè¯´æ˜
# =============================================================================

def demo_multi_feature_labeling():
    """
    æ¼”ç¤ºåŸºäºå¤šç‰¹å¾çš„æ ‡ç­¾ç”Ÿæˆç³»ç»Ÿ
    """
    print("ğŸš€ å¤šç‰¹å¾æ ‡ç­¾ç”Ÿæˆç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # ç¤ºä¾‹ç”¨æ³•
    usage_example = """
    # 1. ç”ŸæˆåŸºç¡€ç‰¹å¾
    df_with_features = build_features_safe(raw_data)
    
    # 2. ä½¿ç”¨å¤šç‰¹å¾æ ‡ç­¾ç”Ÿæˆç³»ç»Ÿ
    df_with_multi_labels = build_multi_feature_labels(
        df_with_features, 
        forward_n=4,        # é¢„æµ‹æœªæ¥4ä¸ªå‘¨æœŸ
        base_thr=0.008      # åŸºç¡€é˜ˆå€¼0.8%
    )
    
    # 3. ç‰¹å¾é€‰æ‹©ï¼ˆå¯é€‰ï¼‰
    final_df = select_important_features(df_with_multi_labels, top_k=25)
    
    # 4. æŸ¥çœ‹ç»“æœ
    print(f"ğŸ‰ æœ€ç»ˆæ•°æ®é›†: {final_df.shape}")
    print(f"ğŸ“Š ä¸»æ ‡ç­¾åˆ†å¸ƒ: {final_df['y'].value_counts(normalize=True).to_dict()}")
    
    # 5. æ¯”è¾ƒä¸åŒæ ‡ç­¾æ–¹æ³•
    label_cols = [col for col in final_df.columns if col.startswith('y')]
    print("\\nğŸ“ˆ ä¸åŒæ ‡ç­¾æ–¹æ³•å¯¹æ¯”:")
    for col in label_cols:
        if col in final_df.columns:
            ratio = final_df[col].mean()
            print(f"  {col}: {ratio:.2%} æ­£æ ·æœ¬æ¯”ä¾‹")
    """
    
    print("ğŸ“– ä½¿ç”¨ç¤ºä¾‹:")
    print(usage_example)
    
    print("\nğŸ”§ ç³»ç»Ÿç‰¹æ€§:")
    print("  âœ… è‡ªåŠ¨ç‰¹å¾æƒé‡è®¡ç®—")
    print("  âœ… åŠ¨æ€é˜ˆå€¼è°ƒæ•´")
    print("  âœ… å¤šæ–¹æ³•æ ‡ç­¾é›†æˆ")
    print("  âœ… æ ‡ç­¾è´¨é‡è¯„ä¼°")
    print("  âœ… é¿å…æ•°æ®æ³„éœ²")
    
    print("\nğŸ“Š æ”¯æŒçš„æ ‡ç­¾æ–¹æ³•:")
    print("  ğŸ¯ y_score_based: åŸºäºç»¼åˆè¯„åˆ†çš„æ ‡ç­¾")
    print("  ğŸ¯ y_dynamic: åŠ¨æ€é˜ˆå€¼æ ‡ç­¾")
    print("  ğŸ¯ y_multi_condition: å¤šæ¡ä»¶ç»„åˆæ ‡ç­¾")
    print("  ğŸ¯ y_relative_strength: ç›¸å¯¹å¼ºåº¦æ ‡ç­¾")
    print("  ğŸ¯ y_vol_adjusted: æ³¢åŠ¨ç‡è°ƒæ•´æ ‡ç­¾")
    
    print("\nğŸ’¡ å‚æ•°è°ƒä¼˜å»ºè®®:")
    print("  ğŸ“ˆ forward_n: 3-7 (é¢„æµ‹å‘¨æœŸ)")
    print("  ğŸ“ˆ base_thr: 0.005-0.015 (åŸºç¡€é˜ˆå€¼)")
    print("  ğŸ“ˆ top_k: 20-40 (ç‰¹å¾é€‰æ‹©æ•°é‡)")


def compare_labeling_methods(df: pd.DataFrame, forward_n: int = 4, base_thr: float = 0.008):
    """
    æ¯”è¾ƒä¸åŒæ ‡ç­¾ç”Ÿæˆæ–¹æ³•çš„æ•ˆæœ
    
    å‚æ•°:
    - df: åŒ…å«ç‰¹å¾çš„DataFrame
    - forward_n: é¢„æµ‹å‘¨æœŸ
    - base_thr: åŸºç¡€é˜ˆå€¼
    """
    print("ğŸ” æ ‡ç­¾æ–¹æ³•å¯¹æ¯”åˆ†æ")
    print("=" * 50)
    
    # ç”Ÿæˆä¸åŒæ–¹æ³•çš„æ ‡ç­¾
    methods = {
        'ä¼ ç»Ÿæ–¹æ³•': build_labels_safe(df, forward_n, base_thr),
        'å¤šç‰¹å¾æ–¹æ³•': build_multi_feature_labels(df, forward_n, base_thr)
    }
    
    comparison_results = {}
    
    for method_name, labeled_df in methods.items():
        print(f"\nğŸ“Š {method_name} ç»“æœ:")
        
        # åŸºæœ¬ç»Ÿè®¡
        total_samples = len(labeled_df)
        positive_samples = (labeled_df['y'] == 1).sum()
        positive_ratio = positive_samples / total_samples
        
        # è®¡ç®—æœªæ¥æ”¶ç›Šç‡ç”¨äºè¯„ä¼°
        future_return = labeled_df['close'].pct_change(forward_n).shift(-forward_n)
        
        # ç›¸å…³æ€§åˆ†æ
        correlation = labeled_df['y'].corr(future_return)
        
        # å‡†ç¡®ç‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        accuracy = (labeled_df['y'] == (future_return > 0)).mean()
        
        comparison_results[method_name] = {
            'total_samples': total_samples,
            'positive_ratio': positive_ratio,
            'correlation': correlation,
            'accuracy': accuracy
        }
        
        print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"  æ­£æ ·æœ¬æ¯”ä¾‹: {positive_ratio:.2%}")
        print(f"  ä¸æœªæ¥æ”¶ç›Šç‡ç›¸å…³æ€§: {correlation:.4f}")
        print(f"  é¢„æµ‹å‡†ç¡®ç‡: {accuracy:.4f}")
    
    # æ¨èæœ€ä½³æ–¹æ³•
    print("\nğŸ† æ–¹æ³•æ¨è:")
    best_method = max(comparison_results.items(), 
                     key=lambda x: abs(x[1]['correlation']) * x[1]['accuracy'])
    
    print(f"  æ¨èæ–¹æ³•: {best_method[0]}")
    print(f"  ç»¼åˆè¯„åˆ†: {abs(best_method[1]['correlation']) * best_method[1]['accuracy']:.4f}")
    
    return comparison_results


# ä½¿ç”¨å»ºè®®
if __name__ == "__main__":
    # æ¼”ç¤ºå¤šç‰¹å¾æ ‡ç­¾ç”Ÿæˆç³»ç»Ÿ
    demo_multi_feature_labeling()
    
    print("\n" + "=" * 60)
    print("ğŸ“ å®Œæ•´ä½¿ç”¨æµç¨‹:")
    print("""
    # 1. ç”Ÿæˆç‰¹å¾
    df_with_features = build_features_safe(raw_data)
    
    # 2. ç”Ÿæˆå¤šç‰¹å¾æ ‡ç­¾
    df_with_multi_labels = build_multi_feature_labels(
        df_with_features, 
        forward_n=4,        # é¢„æµ‹æœªæ¥4ä¸ªå‘¨æœŸ
        base_thr=0.008      # åŸºç¡€é˜ˆå€¼0.8%
    )
    
    # 3. ç‰¹å¾é€‰æ‹©
    final_df = select_important_features(df_with_multi_labels, top_k=25)
    
    # 4. æ•°æ®æ³„éœ²æ£€æµ‹
    detect_data_leakage(final_df, target_col='y')
    
    # 5. æ–¹æ³•å¯¹æ¯”ï¼ˆå¯é€‰ï¼‰
    comparison_results = compare_labeling_methods(df_with_features)
    
    print(f"ğŸ‰ æœ€ç»ˆæ•°æ®é›†: {final_df.shape}")
    print(f"ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ: {final_df['y'].value_counts(normalize=True).to_dict()}")
    """)    