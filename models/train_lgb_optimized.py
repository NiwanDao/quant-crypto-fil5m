import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import roc_auc_score, precision_recall_curve, f1_score
import joblib
import os
import optuna
from typing import Tuple, Dict, List
import yaml
import json
from datetime import datetime
import matplotlib.pyplot as plt

DATA_PATH = "data/feat_2024_8_to_2025_3.parquet"
MODEL_PATH = "models/lgb_trend_optimized.txt"
SKLEARN_DUMP = "models/lgb_trend_optimized.pkl"
ENSEMBLE_MODELS_PATH = "models/ensemble_models.pkl"
THRESHOLDS_PATH = "models/optimal_thresholds.json"
CONF_PATH = "conf/config.yml"

plt.rcParams["font.sans-serif"] = ["SimHei"]  # è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆæˆ– 'Microsoft YaHei'ï¼‰
plt.rcParams["axes.unicode_minus"] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜


def load_conf():
    with open(CONF_PATH, "r") as f:
        return yaml.safe_load(f)


def optimize_hyperparameters(X: pd.DataFrame, y: pd.Series, n_trials: int = 50) -> Dict:
    """ä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ– - ä¿®å¤ç‰ˆæœ¬"""
    print("ğŸ” å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–...")

    def objective(trial):
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 100, 1000),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2, log=True),
            "num_leaves": trial.suggest_int("num_leaves", 20, 100),
            "subsample": trial.suggest_float("subsample", 0.7, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.7, 1.0),
            "max_depth": trial.suggest_int("max_depth", 3, 12),
            "min_child_samples": trial.suggest_int("min_child_samples", 10, 100),
            "reg_alpha": trial.suggest_float("reg_alpha", 0.0, 2.0),
            "reg_lambda": trial.suggest_float("reg_lambda", 0.0, 2.0),
            "random_state": 42,
            "verbose": -1,
            "n_jobs": -1,
        }

        # ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=3)
        scores = []

        for train_idx, val_idx in tscv.split(X):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„è®­ç»ƒæ–¹å¼
            mdl = lgb.LGBMClassifier(**params)
            mdl.fit(
                X_train,
                y_train,
                eval_set=[(X_val, y_val)],
                eval_metric="auc",
                callbacks=[lgb.early_stopping(30)],
            )

            proba = mdl.predict_proba(X_val)[:, 1]
            auc = roc_auc_score(y_val, proba)
            scores.append(auc)

        return np.mean(scores)

    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=n_trials)

    print(f"âœ… æœ€ä¼˜AUC: {study.best_value:.4f}")
    print(f"ğŸ“Š æœ€ä¼˜å‚æ•°: {study.best_params}")

    return study.best_params


def optimize_thresholds(y_true: np.ndarray, y_proba: np.ndarray) -> Tuple[float, float]:
    """æ”¹è¿›çš„ä¹°å…¥/å–å‡ºé˜ˆå€¼ä¼˜åŒ–"""
    print("ğŸ¯ ä¼˜åŒ–äº¤æ˜“é˜ˆå€¼ï¼ˆåŠ¨æ€/åˆ†ä½æ•°æ³•ï¼‰...")

    # ä¹°å…¥é˜ˆå€¼: æœ€å¤§åŒ–F1
    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    best_buy_threshold = thresholds[np.argmax(f1_scores[: len(thresholds)])]

    # å–å‡ºé˜ˆå€¼: å–ä¹°å…¥æ¦‚ç‡çš„ä½åˆ†ä½
    best_sell_threshold = np.percentile(y_proba, 30)  # å¯è°ƒ

    print(f"ğŸ“ˆ æœ€ä¼˜ä¹°å…¥é˜ˆå€¼: {best_buy_threshold:.4f}")
    print(f"ğŸ“‰ æœ€ä¼˜å–å‡ºé˜ˆå€¼: {best_sell_threshold:.4f}")

    return best_buy_threshold, best_sell_threshold


def train_ensemble_models(
    X: pd.DataFrame, y: pd.Series, best_params: Dict, n_models: int = 5
) -> List:
    """è®­ç»ƒé›†æˆæ¨¡å‹ - ä¿®å¤ç‰ˆæœ¬"""
    print(f"ğŸ¤– è®­ç»ƒ{n_models}ä¸ªé›†æˆæ¨¡å‹...")

    models = []
    tscv = TimeSeriesSplit(n_splits=5)

    for seed in range(n_models):
        print(f"è®­ç»ƒæ¨¡å‹ {seed + 1}/{n_models}")

        # ä¸ºæ¯ä¸ªæ¨¡å‹ä½¿ç”¨ä¸åŒçš„éšæœºç§å­
        params = best_params.copy()
        params["random_state"] = seed * 42  # ä¸åŒçš„éšæœºç§å­

        best_model, best_auc = None, -1.0

        for fold, (tr, va) in enumerate(tscv.split(X), start=1):
            Xtr, Xva = X.iloc[tr], X.iloc[va]
            ytr, yva = y.iloc[tr], y.iloc[va]

            mdl = lgb.LGBMClassifier(**params)

            # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å‚æ•°å
            mdl.fit(
                Xtr,
                ytr,
                eval_set=[(Xva, yva)],
                eval_metric="auc",  # æˆ– 'logloss'
                callbacks=[lgb.early_stopping(30)],  # 30è½®æœªæå‡å°±åœæ­¢
            )

            proba = mdl.predict_proba(Xva)[:, 1]
            auc = roc_auc_score(yva, proba)

            if auc > best_auc:
                best_auc, best_model = auc, mdl

        models.append(best_model)
        print(f"æ¨¡å‹ {seed + 1} æœ€ä½³AUC: {best_auc:.4f}")

    return models


def ensemble_predict(models: List, X: pd.DataFrame) -> np.ndarray:
    """é›†æˆæ¨¡å‹é¢„æµ‹"""
    predictions = []
    for model in models:
        pred = model.predict_proba(X)[:, 1]
        predictions.append(pred)

    # è¿”å›å¹³å‡é¢„æµ‹ç»“æœ
    return np.mean(predictions, axis=0)


def save_optimization_results(
    best_params: Dict, thresholds: Tuple[float, float], models: List, conf: Dict
):
    """ä¿å­˜ä¼˜åŒ–ç»“æœ"""
    os.makedirs("models", exist_ok=True)

    # ä¿å­˜æœ€ä¼˜å‚æ•°
    with open("models/best_params.json", "w") as f:
        json.dump(best_params, f, indent=2)

    # ä¿å­˜æœ€ä¼˜é˜ˆå€¼
    thresholds_dict = {
        "buy_threshold": float(thresholds[0]),
        "sell_threshold": float(thresholds[1]),
        "optimized_at": datetime.now().isoformat(),
    }
    with open(THRESHOLDS_PATH, "w") as f:
        json.dump(thresholds_dict, f, indent=2)

    # ä¿å­˜é›†æˆæ¨¡å‹
    joblib.dump(models, ENSEMBLE_MODELS_PATH)

    # ä¿å­˜ä¸»æ¨¡å‹ï¼ˆç¬¬ä¸€ä¸ªæ¨¡å‹ï¼‰
    main_model = models[0]
    main_model.booster_.save_model(MODEL_PATH)
    joblib.dump(main_model, SKLEARN_DUMP)

    # æ›´æ–°é…ç½®æ–‡ä»¶
    conf["model"].update(
        {
            "proba_threshold": float(thresholds[0]),
            "sell_threshold": float(thresholds[1]),
            "use_ensemble": True,
            "n_ensemble_models": len(models),
            "optimized_at": datetime.now().isoformat(),
        }
    )

    with open(CONF_PATH, "w") as f:
        yaml.dump(conf, f, default_flow_style=False)

    print("ğŸ’¾ ä¼˜åŒ–ç»“æœå·²ä¿å­˜")


# å¯è§†åŒ–æ¨¡å‹é€€åŒ–
# è‹¥å¤šæ¬¡è®­ç»ƒå stability_gap æŒç»­æ‰©å¤§ â†’ æ¨¡å‹é€æ¸æ— æ³•ç¨³å®šä¼ é€’ä¿¡å·ï¼Œéœ€è¦é‡è®­ã€‚


# è‡ªåŠ¨ç›‘æ§é˜ˆå€¼ä½“ç³»
# è‹¥ mean_disagreementï¼ˆåˆ†æ­§ç‡ï¼‰çªç„¶å‡é«˜ â†’ å¸‚åœºç»“æ„å˜åŒ–æˆ–ç‰¹å¾ä¸å†æœ‰æ•ˆ
def validate_model_stability(
    models,
    X: pd.DataFrame,
    y: pd.Series,
    *,
    n_splits: int = 5,
    window_size: int | None = None,
    use_spearman: bool = True,
    buy_threshold: float = 0.6,
    sell_threshold: float = 0.4,
    clip_proba_eps: float = 1e-6,
) -> Dict:
    """
    é¢å‘ FIL 15m äº¤æ˜“çš„æ¨¡å‹ç¨³å®šæ€§è¯„ä¼°ï¼ˆä¸é‡è®­ï¼‰
    - ä»¥æ—¶é—´åºåˆ—æ»šåŠ¨çª—å£è¯„ä¼°ï¼šé›†æˆä¸å•æ¨¡å‹çš„ AUC/PR-AUC/Brier/KS
    - å¤šæ ·æ€§ï¼šæˆå‘˜æ¨¡å‹é¢„æµ‹çš„ç›¸å…³æ€§(ç§©ç›¸å…³) â†’ Fisher z å¹³å‡ â†’ åç›¸å…³ 1 - rÌ„
    - ä¸€è‡´æ€§ï¼šstd_aucï¼ˆå•æ¨¡å‹ AUC çš„æ ‡å‡†å·®ï¼‰
    - åˆ†æ­§ç‡ï¼šdisagreementï¼ˆåŸºäºä¹°/å–é˜ˆå€¼çš„ä¿¡å·å·®å¼‚åº¦ï¼‰

    å‚æ•°
    ----
    models: List[sklearn-like]
        å·²è®­ç»ƒå¥½çš„ LightGBM æ¨¡å‹åˆ—è¡¨
    X, y:
        åŒä¸€æ—¶é—´è½´ä¸‹çš„ç‰¹å¾ä¸æ ‡ç­¾ï¼ˆç´¢å¼•éœ€æŒ‰æ—¶é—´å‡åºï¼‰
    n_splits: int
        æ»šåŠ¨åˆ’åˆ†çš„æŠ˜æ•°ï¼ˆä¸ window_size äºŒé€‰ä¸€ï¼Œä¼˜å…ˆ window_sizeï¼‰
    window_size: int | None
        éªŒè¯çª—å£é•¿åº¦ï¼ˆæ ·æœ¬æ•°ï¼‰ï¼›è‹¥æä¾›ï¼Œå°†ä»¥å›ºå®šæ»‘çª—è¯„ä¼°
    use_spearman: bool
        True ç”¨ Spearmanï¼ˆç§©ç›¸å…³ï¼‰ï¼ŒFalse ç”¨ Pearson
    buy_threshold/sell_threshold: float
        ç”¨äºè®¡ç®—åˆ†æ­§ç‡ï¼ˆdisagreementï¼‰
    clip_proba_eps: float
        æ¦‚ç‡è£å‰ªï¼Œé¿å…æç«¯å€¼é€ æˆåº¦é‡ä¸ç¨³å®š

    è¿”å›
    ----
    {
      'mean_auc': float,
      'mean_diversity': float,
      'stability_score': float,           # = mean_auc * mean_diversity * (1 - mean_std_auc)
      'mean_std_auc': float,
      'mean_pr_auc': float,
      'mean_brier': float,
      'mean_ks': float,
      'mean_disagreement': float,
      'by_window': pd.DataFrame           # å„çª—å£æ˜ç»†
    }
    """
    import numpy as np
    import pandas as pd
    from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss
    from scipy.stats import spearmanr, ks_2samp

    assert len(X) == len(y), "X å’Œ y é•¿åº¦ä¸ä¸€è‡´"
    assert len(models) >= 2, "é›†æˆè‡³å°‘éœ€è¦ä¸¤ä¸ªæ¨¡å‹"
    assert (X.index == y.index).all(), "X ä¸ y çš„ç´¢å¼•ï¼ˆæ—¶é—´è½´ï¼‰éœ€å¯¹é½ä¸”æœ‰åº"

    # ä¸¥æ ¼æŒ‰æ—¶é—´æ’åº
    X = X.sort_index()
    y = y.sort_index().astype(int)

    n = len(X)
    windows = []

    if window_size is not None:
        # å›ºå®šé•¿åº¦æ»‘çª—ï¼šå°½é‡è¦†ç›–å…¨å±€ï¼ˆä¸é‡å æˆ–å…è®¸é€‚åº¦é‡å å¯è‡ªè¡Œæ”¹å†™ï¼‰
        start_points = np.linspace(0, n - window_size, num=max(1, n_splits), dtype=int)
        for s in start_points:
            e = int(s + window_size)
            windows.append((s, e))
    else:
        # æŒ‰æŠ˜æ•°åˆ‡æˆ n_splits ä¸ªé€’å¢æ—¶é—´çª—å£
        edges = np.linspace(0, n, num=n_splits + 1, dtype=int)
        for i in range(n_splits):
            s, e = edges[i], edges[i + 1]
            if e - s > 0:
                windows.append((s, e))

    records = []
    for w_idx, (s, e) in enumerate(windows, start=1):
        Xv, yv = X.iloc[s:e], y.iloc[s:e]
        if len(Xv) == 0:
            continue

        # æˆå‘˜æ¨¡å‹æ¦‚ç‡
        member_probs = []
        member_aucs = []

        for m in models:
            p = m.predict_proba(Xv)[:, 1]
            p = np.clip(p, clip_proba_eps, 1 - clip_proba_eps)
            member_probs.append(p)
            # å•æ¨¡å‹ AUCï¼ˆçª—å£å†…ï¼‰
            try:
                member_aucs.append(roc_auc_score(yv, p))
            except ValueError:
                member_aucs.append(np.nan)

        member_probs = np.array(member_probs)  # shape: [n_models, n_samples]
        ensemble_prob = member_probs.mean(axis=0)

        # ---- é›†æˆæ€§èƒ½æŒ‡æ ‡ï¼ˆçª—å£å†…ï¼‰----
        try:
            auc = roc_auc_score(yv, ensemble_prob)
        except ValueError:
            auc = np.nan

        try:
            pr_auc = average_precision_score(yv, ensemble_prob)
        except ValueError:
            pr_auc = np.nan

        try:
            brier = brier_score_loss(yv, ensemble_prob)
        except ValueError:
            brier = np.nan

        # KS ç»Ÿè®¡ï¼ˆæ­£è´Ÿæ ·æœ¬åˆ†å¸ƒåˆ†ç¦»åº¦ï¼‰ï¼Œè¶Šå¤§è¶Šå¥½
        # è¿™é‡Œç›´æ¥ç”¨ ensemble_prob åœ¨æ­£è´Ÿæ ·æœ¬ä¸Šçš„ä¸¤æ ·æœ¬KS
        pos_scores = ensemble_prob[yv == 1]
        neg_scores = ensemble_prob[yv == 0]
        if len(pos_scores) > 0 and len(neg_scores) > 0:
            ks = ks_2samp(pos_scores, neg_scores).statistic
        else:
            ks = np.nan

        # ---- å¤šæ ·æ€§ï¼ˆæˆå‘˜é—´ç›¸å…³ï¼‰----
        # ç›¸å…³çŸ©é˜µï¼šSpearmanï¼ˆç§©ç›¸å…³ï¼‰æ›´ç¨³å¥ï¼›å¦åˆ™ Pearson
        n_models = member_probs.shape[0]
        corr_mat = np.zeros((n_models, n_models))
        for i in range(n_models):
            for j in range(n_models):
                if i == j:
                    corr_mat[i, j] = 1.0
                    continue
                if use_spearman:
                    r, _ = spearmanr(member_probs[i], member_probs[j])
                else:
                    # Pearson
                    xi = member_probs[i] - member_probs[i].mean()
                    xj = member_probs[j] - member_probs[j].mean()
                    denom = np.sqrt((xi**2).sum()) * np.sqrt((xj**2).sum())
                    r = float((xi * xj).sum() / denom) if denom > 0 else 0.0
                # å¤„ç†å¯èƒ½çš„ nan
                if np.isnan(r):
                    r = 0.0
                corr_mat[i, j] = np.clip(r, -0.999999, 0.999999)

        # Fisher z å˜æ¢åå¹³å‡ï¼Œå†é€†å˜æ¢æ±‚å¹³å‡ç›¸å…³
        # ä»…å¯¹ä¸Šä¸‰è§’éå¯¹è§’åšå¹³å‡
        zs, cnt = 0.0, 0
        for i in range(n_models):
            for j in range(i + 1, n_models):
                r = corr_mat[i, j]
                z = 0.5 * np.log((1 + r) / (1 - r))
                zs += z
                cnt += 1
        if cnt > 0:
            z_mean = zs / cnt
            r_mean = (np.exp(2 * z_mean) - 1) / (np.exp(2 * z_mean) + 1)
        else:
            r_mean = 1.0
        diversity = 1.0 - r_mean  # åç›¸å…³ä½œä¸ºå¤šæ ·æ€§

        # ---- ä¸€è‡´æ€§ï¼ˆstd_aucï¼‰----
        member_aucs = np.array(member_aucs, dtype=float)
        std_auc = np.nanstd(member_aucs)

        # ---- åˆ†æ­§ç‡ï¼ˆdisagreementï¼‰----
        # ç”¨ä¹°/å–é˜ˆå€¼å°†å„æ¨¡å‹æ¦‚ç‡ç¦»æ•£æˆ {+1, 0, -1} ä¿¡å·ï¼Œç»Ÿè®¡ pairwise å·®å¼‚
        # +1: p>buy_threshold, -1: p<sell_threshold, 0: è§‚æœ›
        def probs_to_signal(p):
            sig = np.zeros_like(p, dtype=int)
            sig[p > buy_threshold] = 1
            sig[p < sell_threshold] = -1
            return sig

        member_sigs = np.array(
            [probs_to_signal(p) for p in member_probs]
        )  # [n_models, n_samples]
        # ä¸¤ä¸¤æ¯”è¾ƒå·®å¼‚ç‡ï¼ˆä¸è®¡éƒ½ä¸º0çš„æ ·æœ¬ï¼Œå¯é€‰é¡¹ï¼šæ­¤å¤„çº³å…¥æ•´ä½“ï¼‰
        disagreements = []
        for i in range(n_models):
            for j in range(i + 1, n_models):
                diff = (member_sigs[i] != member_sigs[j]).mean()
                disagreements.append(diff)
        disagreement = float(np.mean(disagreements)) if disagreements else 0.0

        records.append(
            {
                "window_idx": w_idx,
                "start": Xv.index[0],
                "end": Xv.index[-1],
                "auc": float(auc) if auc == auc else np.nan,
                "pr_auc": float(pr_auc) if pr_auc == pr_auc else np.nan,
                "brier": float(brier) if brier == brier else np.nan,
                "ks": float(ks) if ks == ks else np.nan,
                "diversity": float(diversity),
                "std_auc": float(std_auc) if std_auc == std_auc else np.nan,
                "disagreement": float(disagreement),
            }
        )

    by_win = pd.DataFrame.from_records(records)
    if by_win.empty:
        return {
            "mean_auc": np.nan,
            "mean_diversity": np.nan,
            "stability_score": np.nan,
            "mean_std_auc": np.nan,
            "mean_pr_auc": np.nan,
            "mean_brier": np.nan,
            "mean_ks": np.nan,
            "mean_disagreement": np.nan,
            "by_window": by_win,
        }

    mean_auc = by_win["auc"].mean()
    mean_div = by_win["diversity"].mean()
    mean_std_auc = by_win["std_auc"].mean()
    mean_pr_auc = by_win["pr_auc"].mean()
    mean_brier = by_win["brier"].mean()
    mean_ks = by_win["ks"].mean()
    mean_disagreement = by_win["disagreement"].mean()

    # ç»¼åˆç¨³å¥åº¦ï¼šå‡†ç¡®(auc) Ã— å¤šæ ·(diversity) Ã— ä¸€è‡´æ€§(1-std_auc)ï¼ˆå¯æŒ‰éœ€æ”¹æƒé‡ï¼‰
    stability_score = (
        (mean_auc if pd.notna(mean_auc) else 0.0)
        * (mean_div if pd.notna(mean_div) else 0.0)
        * (1.0 - (mean_std_auc if pd.notna(mean_std_auc) else 0.0))
    )

    return {
        "mean_auc": float(mean_auc),
        "mean_diversity": float(mean_div),
        "stability_score": float(stability_score),
        "mean_std_auc": float(mean_std_auc),
        "mean_pr_auc": float(mean_pr_auc),
        "mean_brier": float(mean_brier),
        "mean_ks": float(mean_ks),
        "mean_disagreement": float(mean_disagreement),
        "by_window": by_win,
    }


def save_stability_report(
    stability_model: dict,
    stability_signal: dict,
    report_path: str = "models/stability_report.csv",
):
    """
    ä¿å­˜å¹¶å¯è§†åŒ–ç¨³å®šæ€§æŸè€—æŠ¥å‘Šã€‚
    æ¯”è¾ƒæ¨¡å‹å±‚é¢ä¸ä¿¡å·å±‚é¢ç¨³å®šæ€§å·®å¼‚ï¼Œé•¿æœŸè¿½è¸ªæ¨¡å‹é€€åŒ–ã€‚
    """

    # è®¡ç®—ç¨³å®šæ€§å·®å¼‚
    stability_gap = (
        stability_model["stability_score"] - stability_signal["stability_score"]
    )

    # è®°å½•æ—¶é—´ä¸æŒ‡æ ‡
    record = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "mean_auc_model": stability_model["mean_auc"],
        "mean_div_model": stability_model["mean_diversity"],
        "mean_auc_signal": stability_signal["mean_auc"],
        "mean_div_signal": stability_signal["mean_diversity"],
        "mean_disagreement": stability_signal.get("mean_disagreement", np.nan),
        "std_auc_signal": stability_signal.get("mean_std_auc", np.nan),
        "stability_gap": stability_gap,
        "stability_model_score": stability_model["stability_score"],
        "stability_signal_score": stability_signal["stability_score"],
    }

    # åˆ›å»º/è¿½åŠ æŠ¥å‘Šæ–‡ä»¶
    os.makedirs(os.path.dirname(report_path), exist_ok=True)
    if os.path.exists(report_path):
        df = pd.read_csv(report_path)
        df = pd.concat([df, pd.DataFrame([record])], ignore_index=True)
    else:
        df = pd.DataFrame([record])

    df.to_csv(report_path, index=False)
    print(f"ğŸ’¾ ç¨³å®šæ€§æŠ¥å‘Šå·²æ›´æ–°: {report_path}")

    # å¯è§†åŒ–è¶‹åŠ¿ï¼ˆå¯é€‰ï¼‰
    if len(df) > 2:
        plt.figure(figsize=(10, 6))
        plt.plot(
            df["timestamp"],
            df["stability_model_score"],
            label="æ¨¡å‹å±‚é¢ç¨³å®šæ€§",
            marker="o",
        )
        plt.plot(
            df["timestamp"],
            df["stability_signal_score"],
            label="ä¿¡å·å±‚é¢ç¨³å®šæ€§",
            marker="x",
        )
        plt.plot(
            df["timestamp"],
            df["stability_gap"],
            label="ç¨³å®šæ€§æŸè€—",
            linestyle="--",
            color="gray",
        )
        plt.xticks(rotation=45)
        plt.xlabel("æ—¶é—´")
        plt.ylabel("å¾—åˆ† / ç¨³å®šæ€§å·®")
        plt.title("ğŸ“Š æ¨¡å‹ç¨³å®šæ€§ä¸ä¿¡å·ä¸€è‡´æ€§è¶‹åŠ¿")
        plt.legend()
        plt.tight_layout()
        plt.savefig("models/stability_trend.png")
        plt.close()
        print("ğŸ“ˆ ç”Ÿæˆè¶‹åŠ¿å›¾: models/stability_trend.png")


# ========= è¯„ä¼°å‡½æ•°ï¼ˆVal/Testé€šç”¨ï¼‰=========
def evaluate_split(y_true, proba, buy_th, sell_th):
    from sklearn.metrics import (
        roc_auc_score,
        average_precision_score,
        brier_score_loss,
        f1_score,
        precision_score,
        recall_score,
    )
    import numpy as np

    # åŒºåˆ† + æ ¡å‡†
    auc = roc_auc_score(y_true, proba)
    pr_auc = average_precision_score(y_true, proba)
    brier = brier_score_loss(y_true, proba)

    # é˜ˆå€¼ä¿¡å·
    long_sig = (proba > buy_th).astype(int)
    short_sig = (proba < sell_th).astype(int)

    # åˆ†ç±»æŒ‡æ ‡ï¼ˆç¤ºä¾‹ï¼šåšå¤šä¿¡å·ï¼‰
    f1_long = f1_score(y_true, long_sig, zero_division=0)
    prec_long = precision_score(y_true, long_sig, zero_division=0)
    rec_long = recall_score(y_true, long_sig, zero_division=0)

    # Lift@5%ï¼šå–Top 5% æ¦‚ç‡çš„æ ·æœ¬
    k = max(1, int(0.05 * len(proba)))
    topk_idx = np.argsort(proba)[-k:]
    lift_at_5p = (
        float(y_true.iloc[topk_idx].mean() / y_true.mean())
        if y_true.mean() > 0
        else np.nan
    )

    # TODO: è‹¥ä½ æœ‰å›æµ‹å‡½æ•°ï¼Œå¯åœ¨è¿™é‡ŒåŠ å‡€æ”¶ç›Šã€Sharpeã€MaxDDç­‰
    # pnl = backtest_net_metrics(long_sig, short_sig, ...)

    return {
        "auc": float(auc),
        "pr_auc": float(pr_auc),
        "brier": float(brier),
        "f1_long": float(f1_long),
        "prec_long": float(prec_long),
        "rec_long": float(rec_long),
        "lift_at_5p": float(lift_at_5p),
        # **pnl
    }


def main():
    print("ğŸš€ å¼€å§‹LightGBMæ¨¡å‹ä¼˜åŒ–...")

    # åŠ è½½é…ç½®
    conf = load_conf()

    # åˆ†åˆ«åŠ è½½è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®
    train_df = pd.read_parquet("data/feat_2024_8_to_2025_3.parquet")
    val_df = pd.read_parquet("data/feat_2025_3_to_2025_6.parquet")
    test_df = pd.read_parquet("data/feat_2025_6_to_now.parquet")

    # è®­ç»ƒé›†ç”¨äºè®­ç»ƒ
    train_features = [c for c in train_df.columns if c not in ["y"]]
    X_train, y_train = train_df[train_features], train_df["y"]

    # éªŒè¯é›†ç”¨äºè¶…å‚æ•°ä¼˜åŒ–å’Œæ—©åœ
    val_features = [c for c in val_df.columns if c not in ["y"]]
    X_val, y_val = val_df[val_features], val_df["y"]

    # æµ‹è¯•é›†ç”¨äºæœ€ç»ˆè¯„ä¼°
    test_features = [c for c in test_df.columns if c not in ["y"]]
    X_test, y_test = test_df[test_features], test_df["y"]

    print(f"ğŸ“Š è®­ç»ƒé›†: {len(train_df)} æ¡è®°å½•")
    print(f"ğŸ“Š éªŒè¯é›†: {len(val_df)} æ¡è®°å½•")
    print(f"ğŸ“Š æµ‹è¯•é›†: {len(test_df)} æ¡è®°å½•")

    # æ•°æ®è´¨é‡æ£€æŸ¥
    if train_df.isnull().sum().sum() > 0:
        print("âš ï¸ è®­ç»ƒæ•°æ®åŒ…å«ç©ºå€¼ï¼Œè¿›è¡Œæ¸…ç†...")
        train_df = train_df.dropna()
    if val_df.isnull().sum().sum() > 0:
        print("âš ï¸ éªŒè¯æ•°æ®åŒ…å«ç©ºå€¼ï¼Œè¿›è¡Œæ¸…ç†...")
        val_df = val_df.dropna()
    if test_df.isnull().sum().sum() > 0:
        print("âš ï¸ æµ‹è¯•æ•°æ®åŒ…å«ç©ºå€¼ï¼Œè¿›è¡Œæ¸…ç†...")
        test_df = test_df.dropna()

    # 1. è¶…å‚æ•°ä¼˜åŒ–
    best_params = optimize_hyperparameters(X_train, y_train, n_trials=30)

    # 2. è®­ç»ƒé›†æˆæ¨¡å‹
    # ç”¨åŒä¸€å¥—æœ€ä¼˜è¶…å‚æ•°ï¼ˆbest_paramsï¼‰ï¼ŒåŸºäºæ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼ˆTimeSeriesSplitï¼‰åå¤è®­ç»ƒå¤šä¸ªLightGBMæ¨¡å‹ï¼ˆä½¿ç”¨ä¸åŒéšæœºç§å­ï¼‰ï¼Œå¹¶æŒ‘é€‰å‡ºæ¯ä¸ªéšæœºç§å­çš„æœ€ä¼˜å­æ¨¡å‹ï¼Œç»„æˆä¸€ä¸ªæ¨¡å‹é›†æˆã€‚
    models = train_ensemble_models(X_train, y_train, best_params, n_models=5)

    # 3. ç¨³å®šæ€§è¯„ä¼° â€”â€” é˜¶æ®µä¸€ï¼šæ¨¡å‹å±‚é¢ï¼ˆä¸ä¾èµ–é˜ˆå€¼ï¼‰
    print("\nğŸ“˜ é˜¶æ®µä¸€ï¼šæ¨¡å‹å±‚é¢ç¨³å®šæ€§ï¼ˆç»“æ„ä¸€è‡´æ€§ï¼‰")
    stability_model = validate_model_stability(
        models, X_train, y_train, n_splits=5, use_spearman=True
    )

    # 4. é˜ˆå€¼ä¼˜åŒ–
    # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´åºåˆ—åˆ†å‰²è¿›è¡Œé˜ˆå€¼ä¼˜åŒ–
    tscv = TimeSeriesSplit(n_splits=5)
    train_idx, val_idx = list(tscv.split(X_train))[-1]
    X_val_for_thresh, y_val_for_thresh = X_train.iloc[val_idx], y_train.iloc[val_idx]

    y_proba = ensemble_predict(models, X_val_for_thresh)
    buy_threshold, sell_threshold = optimize_thresholds(
        y_val_for_thresh.values, y_proba
    )

    # 5. ç¨³å®šæ€§è¯„ä¼° â€”â€” é˜¶æ®µäºŒï¼šä¿¡å·å±‚é¢ï¼ˆå¸¦é˜ˆå€¼ï¼‰
    print("\nğŸ“™ é˜¶æ®µäºŒï¼šä¿¡å·å±‚é¢ç¨³å®šæ€§ï¼ˆå®ç›˜ä¸€è‡´æ€§ï¼‰")
    stability_signal = validate_model_stability(
        models,
        X_train,
        y_train,
        n_splits=5,  # æ—¶é—´åºåˆ—åˆ†å‰²æ•°é‡ï¼ˆæˆ–ç”¨ window_size æ§åˆ¶çª—å£é•¿åº¦ï¼‰
        use_spearman=True,  # ä½¿ç”¨ Spearman ç§©ç›¸å…³ï¼ŒæŠ—å™ªæ›´å¼º
        buy_threshold=buy_threshold,  # ä½¿ç”¨ä½ åˆšæ‰ä¼˜åŒ–å¾—åˆ°çš„ä¹°å…¥é˜ˆå€¼
        sell_threshold=sell_threshold,  # ä½¿ç”¨ä½ åˆšæ‰ä¼˜åŒ–å¾—åˆ°çš„å–å‡ºé˜ˆå€¼
    )

    # 8. æ¯”è¾ƒä¸¤ä¸ªé˜¶æ®µ
    # è‡ªåŠ¨ç”Ÿæˆè¶‹åŠ¿å›¾ï¼š
    # è“çº¿ï¼šæ¨¡å‹å±‚é¢ç¨³å®šæ€§ï¼ˆç»“æ„ç¨³å®šï¼‰
    # æ©™çº¿ï¼šä¿¡å·å±‚é¢ç¨³å®šæ€§ï¼ˆå®ç›˜æ‰§è¡Œï¼‰
    # ç°è™šçº¿ï¼šä¸¤è€…å·®è·ï¼ˆç¨³å®šæ€§æŸè€—ï¼‰

    save_stability_report(stability_model, stability_signal)

    # 9 â€œæœ€ç»ˆè¯„ä¼°â€æ‘˜è¦
    print("\n" + "=" * 50)
    print(" è®­ç»ƒé˜¶æ®µæœ€ç»ˆæ‘˜è¦")
    print("=" * 50)
    print(f" æ¨¡å‹ç¨³å®šæ€§åˆ†æ•°: {stability_signal['stability_score']:.4f}")
    print(f" æœ€ä¼˜ä¹°å…¥é˜ˆå€¼: {buy_threshold:.4f} | ğŸ“‰ æœ€ä¼˜å–å‡ºé˜ˆå€¼: {sell_threshold:.4f}")

    # 10 éªŒè¯é›†è¯„ä¼°ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
    print("\n" + "=" * 30)
    print("ğŸ“™ éªŒè¯é›†è¯„ä¼°ï¼ˆç”¨äºå¯¹æ¯”ï¼‰")
    print("=" * 30)
    val_proba = ensemble_predict(models, X_val)
    val_metrics = evaluate_split(y_val, val_proba, buy_threshold, sell_threshold)
    print(" | ".join([f"{k}:{v:.4f}" for k, v in val_metrics.items()]))

    # 11 æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°ï¼ˆé”å‚é”é˜ˆå€¼ï¼‰
    print("\n" + "=" * 30)
    print("ğŸ¯ æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°ï¼ˆé”å‚/é”é˜ˆå€¼ï¼‰")
    print("=" * 30)
    test_proba = ensemble_predict(models, X_test)
    test_metrics = evaluate_split(y_test, test_proba, buy_threshold, sell_threshold)
    print(" | ".join([f"{k}:{v:.4f}" for k, v in test_metrics.items()]))

    # 12 Val vs Test å¯¹æ¯”ï¼ˆæ³›åŒ–å·®ï¼‰
    print("\nğŸ§­ å·®å¼‚ï¼ˆTest - Valï¼‰")
    for k in val_metrics:
        if k in test_metrics:
            try:
                print(f"{k}: {test_metrics[k]-val_metrics[k]:+.4f}")
            except Exception:
                pass

    # 13. ä¿å­˜ä¼˜åŒ–ç»“æœï¼ˆå‚æ•°/é˜ˆå€¼/é›†æˆï¼‰
    save_optimization_results(
        best_params, (buy_threshold, sell_threshold), models, conf
    )

    print("\nâœ… æ¨¡å‹ä¼˜åŒ–ä¸è¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()
